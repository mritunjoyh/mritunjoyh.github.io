---
---
@article{pal2024transmission,
  title={A transmission model based deep neural network for image dehazing},
  author={Pal, Tannistha and Halder, Mritunjoy and Barua, Sattwik},
  journal={Multimedia Tools and Applications},
  volume={83},
  number={13},
  pages={39255--39281},
  year={2024},
  preview={MTApp.png},
  abstract={In recent years, one of the main contributors to traffic fatalities and poor vision in poor weather conditions has been rising. In order to lower accident rates, improving visibility is essential for driver assistance systems, image acquisition, and surveillance systems. Thus, as a result, this research introduces a novel Deep Neural Network (DNN) based on a scattering model for defogging an image that has been affected with fog. The proposed model consists of two components: the fog dilution model and the diluted fog removal model. In addition, we have incorporated a deep learning-based transmission estimation module. The fog dilution model is designed to mitigate the presence of fog, while the diluted fog removal model aims to completely eliminate the remaining fog. To enhance the accuracy of transmission estimation, we employ a green channel prior-based approach. This approach effectively reduces the distortion caused in the sky in the resulting defogged image. By combining these components, our model offers a comprehensive solution for fog removal, with improved image quality and reduced sky artifacts. In addition to addressing the shortcomings of current vision enhancement techniques as mentioned in the paper, the proposed method also satisfies human perceptual needs. Experimental results have demonstrated that the proposed model delivers superior visibility enhancement results for both non-reference and full-reference metrics following a qualitative comparison with eight state-of-the-art defogging techniques. The experimental results have proven that the proposed method achieves superior performance in terms of qualitative evaluation on non-reference metric (i.e., in terms of e = 0.50, = 0.15, r = 1.49) and reference metric (i.e. in terms of MSE= 397.16, PSNR = 23.38, NCC = 0.99, MD = 24.39, NAE = 0.13) compared with eight state-of-the-art dehazing methods. Furthermore, based on the average computational time achieved by the proposed method (0.17 s using HSTS dataset), it can be highly suitable for real-time applications. The suggested approach could therefore be employed as a viable route for improving vision in surveillance and acquisition systems while lowering the risk to users’ safety.},
  doi={https://doi.org/10.1007/s11042-023-17010-4},
  selected={true},
  publisher={Springer}
}
@inproceedings{halder2023anomalous,
  title={Anomalous activity detection from ego view camera of surveillance robots},
  author={Halder, Mritunjoy and Banerjee, Snehasis and Purushothaman, Balamuralidhar},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2023},
  preview={ijcnn.png},
  abstract={Can a surveillance robot autonomously detect anomalous activity from its ego view camera perception? This is a challenging task as it requires identifying what is normal and what is an abnormal pattern - given the variations of possible anomalies and abnormalities. This paper presents an architecture and method based on a spatio-temporal convolution neural network to detect and classify anomalies. This work is inspired by the ‘Konio-Magno-Parvocellular’ cells of the human brain, which is claimed to aid humans in organizing changes in perceived scenes. The model is trained and tested on a benchmark video dataset of human activity. We have obtained 91% testing accuracy on this dataset. Experiments in simulation as well as deployment on a real robot shows that the proposed methodology can identify anomalous activities effectively. We have also listed down the observations from practical deployment of the model.},
  doi={https://doi.org/10.1109/IJCNN54540.2023.10191611},
  selected={true},
  organization={IEEE}
}
@inproceedings{barua2022framework,
  title={A Framework for Sex Identification, Accent and Emotion Recognition from Speech Samples},
  author={Barua, Sattwik and Halder, Mritunjoy and Kumar, Mohit},
  booktitle={2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
  pages={1--7},
  year={2022},
  preview={icccnt.png},
  abstract={The purpose of understanding and interpreting the world lies in the cognitive system of a living body. The difficulties that humans face in acknowledging cognitive senses and abilities are only because of the singularity of our views on human cognition. If animals have feelings, there might be a case to be made that robots do have. Non-expert users interacting with robots may mislead the functionality of the robot. If someone is present in front of a robot and the robot can determine their sex, accent of English and emotion simply from their speech data, it helps in development of an embedded cognitive model. Though having limitations, studies in neuroscience have shown that animal models have significantly contributed to the understanding of the mechanistic and functional aspects of cognitive activities such as speaking. As a result, the usage of robots in a range of industries requires the capacity to make understandable and expressive speech. Thus, we present three models for identifying sex, accent, and emotion. To attain better outcomes, we have proposed a Deep Neural Network (DNN) and adjusted it as needed. To evaluate how well the model can perform, we have employed the attention layer, bidirectional Long Short Term Memory (LSTM) and Dropout concepts and judged our trainable datapoints to be very large. The experimental results show our superior findings, in terms of accuracy obtained in identification of sex and accent: 94.62%, 97.37% consecutively and recognition of emotion: 99.84%, which suggest that our study might be applied in the future as a noteworthy solution for the aforementioned problems.},
  doi={https://doi.org/10.1109/ICCCNT54827.2022.9984265},
  selected={true},

  organization={IEEE}
}
@article{pal2023multi,
  title={Multi-feature based hazy image classification for vision enhancement},
  author={Pal, Tannistha and Halder, Mritunjoy and Barua, Sattwik},
  journal={Procedia Computer Science},
  volume={218},
  pages={2653--2665},
  year={2023},
  abstract={Deteriorating contrast, low glow, restricted dynamic range, poor resolution details, non-bright natural landscape colours, and reduced saturation of an image are subjected to various degrees of influence and deterioration in hazy weather conditions. Dehazing haze degraded images becomes challenging if they are not classified as hazy or clear, given that image dehazing techniques can only be used with hazy images. The competence to differentiate between hazy and clear images can not be left to human perception; hence a robust model is needed that classifies the input image into hazy and clear. Thus, we propose a nine unique features-based image classification framework based on K-Nearest Neighbour (KNN), which can accurately classify hazy and clear images. Experimental results demonstrate that the proposed method can efficiently classify the hazy and clear images, with an accuracy of 92%, a precision of 0.90, recall of 0.96, and F1 score of 0.93 for a benchmark dataset, which has both theoretical and practical implications.},
  doi={https://doi.org/10.1016/j.procs.2023.01.238},
  publisher={Elsevier}
}
@article{pal2023deep,
  title={A deep learning model to detect foggy images for vision enhancement},
  author={Pal, Tannistha and Halder, Mritunjoy and Barua, Sattwik},
  journal={The Imaging Science Journal},
  volume={71},
  number={6},
  pages={484--498},
  year={2023},
  abstract={Fog limits meteorological visibility, posing a significant danger to road safety. Poor visibility is considered as a significant contributor to road accidents in foggy weather conditions. Regardless, image defogging techniques can only work with foggy images. In a real-time system, however, defogging foggy images becomes difficult if they are not identified as foggy or clear. Because we cannot rely on human vision to distinguish between foggy and clear pictures, we need a robust model that classifies the input image as foggy or clear based on some features. This paper proposes a robust Deep Learning (DL) model based on Convolutional Neural Network (CNN) for classifying the input as foggy and clear. The proposed Deep Neural Network (DNN) architecture is efficient and precise enough to classify images as foggy or clear, with a training time complexity of O(n2) and a prediction time complexity of O(n). The experimental results reveal promising results in both qualitative and quantitative assessments. The model has an accuracy of 94.8%, precision of 91.8%, recall of 75.8% and F1 score of 80.3% when evaluated on the SOTS dataset, indicating that it might be utilized to mitigate the safety risk in vision enhancement systems.},
  doi={https://doi.org/10.1080/13682199.2023.2185429},
  publisher={Taylor \& Francis}
}
@incollection{barua2023dehazing,
  title={Dehazing and vision enhancement: challenges and future scope},
  author={Barua, Sattwik and Pal, Tannistha and Halder, Mritunjoy},
  booktitle={IET Intelligent Multimedia Processing and Computer Vision},
  publisher={IET},
  abstract={Poor visibility of outdoor images has been drastically increased. Applications using computer vision, including surveillance systems, intelligent transportation systems, are not able to function properly due to limited visibility. Numerous image dehazing methods have been introduced as a solution to this problem, and they are crucial in enhancing the functionality of several computer vision systems. The dehazing approaches are intriguing to researchers as a consequence. In order to demonstrate that dehazing techniques could be successfully used in actual practice, this study conducts an extensive examination of the state-of-the-art dehazing approaches. In contrast, it motivates scholars to apply some of these methods for removing haze from hazy images. In this chapter, we discuss several robust mathematical models along with some neural network-based approaches and their implementations in various aspects. Finally, we address several concerns about difficulties and potential future applications of dehazing approaches.
  Due to poor visibility conditions, the visibility of outdoor images is drastically decreased. Applications using computer vision, including surveillance systems, intelligent transportation systems, etc., are not able to function properly due to limited visibility. Numerous image dehazing methods have been introduced as a solution to this issue, and they are crucial in enhancing the functionality of several computer vision systems. The dehazing approaches are intriguing to researchers as a consequence. In order to demonstrate that dehazing techniques could be successfully used in actual practice, this study conducts an extensive examination of the state-of-the-art dehazing approaches. In contrast, it motivates scholars to apply some of these methods for removing haze from hazy images. We keep an eye on the robust mathematical models along with some neural network-based approaches and their implementations in various aspects. Finally, we address several concerns about difficulties and potential future applications of dehazing approaches.},
  doi={https://doi.org/10.1049/PBPC064E_ch10},
  year={2023} 
}
@misc{banerjee2024anomalous,
  title={Anomalous activity detection for mobile surveillance robots},
  author={Banerjee, Snehasis and Purushothaman, Balamuralidhar and Halder, Mritunjoy},
  year={2024},
  month=may # "~30",
  publisher={Google Patents},
  abstract={Technical challenge in unusual human activity detection task is to rightly identify only unexpected or unusual movements from constant regular movements present in a scene, with most techniques built on understanding that camera is static. However, ego view camera of mobile surveillance robot is in motion as robot navigates. Embodiments herein provide a method and system for anomalous activity detection for mobile surveillance robots by mimicking ‘Konio-Parvocellular-Magno’ cells of the human brain into a NN model, which are responsible for detecting slow, normal, and swift changes in perceived scenes. To detect anomalous activity, the static or normal movements of scene captured by ego view camera are identified as redundant information and only RoI is forwarded for further processing using the Optical flow and SSIM techniques. The NN model mimicking KPM is trained only on the RoI to detect normal or anomalous activity.},
  preview={ijcnn.png},
  doi={https://patents.google.com/patent/US20240177488A1/en},
  note={US Patent App. 18/473,595}
}

@misc{mrit2023revelation,
  title={Detecting Emotional Sentiment in Cartoons},
  author={Mritunjoy Halder, Anuvab Sen, Arirtra Bandyopadhyay, Dibyarup Dutta},
  year={2023},
  publisher={Github},
  month=may # "~30",
  abstract={Social media platforms are widely used by individuals and organizations to express emotions, opinions, and ideas. These platforms generate vast amounts of data, which can be analyzed to gain insights into user behavior, preferences, and sentiment. Accurately classifying the sentiment of social media posts can provide valuable insights for businesses, individuals, and organizations to make informed decisions. To accomplish this task, a customized private cartoon dataset (original images) of social media posts has been provided, which contains labels for each post's emotion category, such as happy, angry, sad, or neutral. The task is to build and fine-tune a machine-learning model that accurately classifies social media posts into their corresponding emotion categories, using synthetic images.},
  doi={https://github.com/AnuvabSen1/TheDataDorks_Revelation_BrainDead_Submissions},
  note={Revelation23}
}
